# Intern-Code-Test Review

## 구현과정에서 생각한 것들

### fast API

#### 도입

전에 fast api를 사용해 정말 간단한 서버를 만들어 본 적이 있었는데, api가 단 한개 뿐인 서버라 딱히 디렉토리 구조를 고민하지 않았었다. 이번 기회에 fast API에서 권장하는 디렉토리 구조를 공부하고 다른 어느 정도 규모가 있는 fast API 프로젝트를 참고하여 디렉토리 구조를 설정했다.

> 참조한 공식문서: https://fastapi.tiangolo.com/ko/tutorial/bigger-applications/ <br>
> 참조한 레포: https://github.com/tiangolo/full-stack-fastapi-postgresql/tree/master <br> > <img src="https://cdn.discordapp.com/attachments/1156230299202625608/1209724211829145680/2024-02-21_1.51.24.png?ex=65e7f677&is=65d58177&hm=33f8471fb162aa2824e87a99ae14ddcbe1a0e6e0e99761936e41a8f557abbcac&" width="350"/>

#### 작업

api.py 에서는 routing을 해주는 파일들을 모아주는 역할만을 부여했다. 실제로 routing을 하는 파일은 endpoints 디렉토리 안의 source들에게 역할을 주었다. 해당 endpoints 안의 source들은 서로 직접적인 의존성을 가지지 않게 설계했다. 이 endpoints 들은 crud 레포지토리의 service들을 호출해 로직을 처리한다. 실제 로직은 crud 디렉토리 안의 source들이 갖도록 했다. 그리고 이 crud source들은 서로 의존성을 가질 수 있게 설계했다. 따로 주입 받아야하는 redis와 rq의 경우에는 처음에는 어디에 의존성을 추가해야할 지 모르겠어서 우선 dependency파일로 빼놓고 필요할 때 import 해서 사용했는데, 참고용 레포지토리를 보니 이런식으로 구성해도 괜찮을 것 같아서 이대로 구현했다.

#### 느낀점

nestJS나 spring과는 조금 다른 디렉토리 구조라서 신기했는데, 찾아보니 express에서 이런 방식으로 라우팅을 하고 디렉토리 구조를 비슷하게 가져가는 것 같았다. 방식의 차이가 조금 있을 뿐 사용해보니 프레임워크 별로 느낌은 비슷했다.

### docker compose

#### 도입

docker compose 에 관해서도 이번에 처음 경험을 해보았는데, 너무나 편리한 도구였다. 여러개의 컨테이너에 각각 필요한 이미지를 넣고 서로 통신하게 하고, 내가 밖으로 노출하고 싶지 않은 포트의 경우에는 숨기고 외부와 소통하고 싶은 포트만 열어둔다던지, 여러개의 프로세스가 동시에 작업을 해야하는 일도 간단하게 처리할 수 있어서 좋았다. 그리고 특히 이를 docker compose 라고 하는 한 파일에 작성 후 docker compose up이라는 명령어를 실행만 하면 바로 환경이 설정되는 부분도 좋았다.

#### 작업

나는 이 docker compose를 통해 worker를 따로 하나의 컨테이너를 더 만들어서 비동기 큐 작업을 진행했다. fast API 서버에서 worker에게 일을 시키고 결과를 기다리지 않고 작업을 종료하면, worker는 다른 컨테이너에서 작업을 처리한다. 그리고 rq worker에 관하여 공식문서를 읽어보았는데, worker는 싱글 스레드 기반이라서 동시에 한 가지 일만 처리할 수 있다고 되어있었다. 그래서 50개의 일을 생성하고 마치는 일을 10초안에 무조건 성공하려면 0 ~ 0.2 간의 sleep을 걸어야 했고, 이 보다 더 오래 sleep 범위를 잡으면 10초 안에 작업을 하는 것이 불가능 했다. 이는 각 job의 ttl을 10초 설정해서 불가능 하다는 것을 확인했다. 혹시 worker를 여러개 만들고 작업을 하면 되지 않을까? 라는 생각이 들어 worker를 3개로 늘리고 sleep 시간을 더 길게 해보았는데 이때는 가능하다는 사실을 확인해 보았다.

#### 추가사항

추가적으로, docker compose라는 것을 통해서 container 끼리 소통하는 모습을 보기도 했고, 최근에 졸업 프로젝트를 MSA 관련 주제로 잡고 공부를 시작하는 과정에 있기도 해서 잠깐 상상을 해보았는데, 내가 최근에 만든 traB 어플의 서버에도 이를 충분히 적용할 수 있을 것이라는 판단이 섰다. 지금은 main server와 sub server가 따로 존재하는 상황이고, 이를 따로 분리했던 이유는 CI/CD 파이프라인 중 docker image생성이 너무 오래걸리기 때문이었다. node 기반으로 만들어진 이미지에 python 관련 dependency를 추가하려면 (특히 pytorch 관련) 대략 25분 이상이 걸려야 한 번의 CI/CD가 완료가 되었다. 그런데 이럴거면 서버와 서버를 분리해서 두개의 instance를 사용하는 것은 너무나 큰 리소스 낭비 같았다. 그리고 어쨌든 rest api로 통신을 해야하는 과정이 하나 추가되는 것이기 때문에 이것으로 인한 시간 지연도 발생할 것으로 우려 되었다. 우리의 메인서버가 고사양의 일을 필요로 하는 것도 아니기에, 하나의 인스턴스에 합쳐서 만들 수 있을 것 같았다 (사실 원래도 그래서 하나의 인스턴스에 진행하려 했었다.) 그리고 마침 우리의 서버는 container 기반으로 배포가 되고 있으니까, container를 하나 더 만들어서 컨테이너끼리 통신하게 하면 되겠다 라고 판단이 섰다. 그래서 이를 docker compose로 로컬에서 한 번 실험을 해보았다.

<img src="https://cdn.discordapp.com/attachments/1156230299202625608/1209922350930464829/2024-02-22_2.49.37.png?ex=65e8aeff&is=65d639ff&hm=c994b20ce1558000401babe643d5db35c95453ddb37d54f002d3242325a91313&" width="350"/>
<img src="https://cdn.discordapp.com/attachments/1156230299202625608/1209922335491100743/2024-02-22_2.49.20.png?ex=65e8aefc&is=65d639fc&hm=3f09600033b1900a90e3cf639c29b3b0bd2c02b9f72b484d98a9c9f0f8449d4f&" width="350"/>
<img src="https://cdn.discordapp.com/attachments/1156230299202625608/1209922316541366344/2024-02-22_2.48.26.png?ex=65e8aef7&is=65d639f7&hm=8c27637d3715050d3a2e99fb2046acb880c5459c54f41182e13d4a0588fa3db6&" width="350"/>
<img src="https://cdn.discordapp.com/attachments/1156230299202625608/1209922301986865213/2024-02-22_2.48.11.png?ex=65e8aef4&is=65d639f4&hm=582b4c6812116a40068e50303791aed2f8224779d1b32b7b2b3bd87d29ff9ed3&" width="350"/>

우선 dockerfile을 적절히 만들어서 YOLO V5 모델을 제공하는 fast api 서버를 만들어서 미리 이미지로 내 로컬 도커에 넣어두었다. 그리고 traB 메인서버는 원래 사용하던 dockerfile을 이용하고 docker compose 명령어로 build 하도록 했다. 그리고 서버 내부에서 axios로 호출하는 부분도 내부에서 연결하는 dns를 이용해서 하도록 변경했다. 어려운 과정은 아니었지만, 공부해서 적용해보았다는 점에서 스스로 조금 뿌듯했다.

### Redis / Redis Queue

#### 도입

redis의 경우에 전전 프로젝트에서 socket 관련으로 리드 개발자분께서 개발하는 일이 있었는데, 그 과정에서 사용하는 걸 얼핏 보아서 개념 정도는 알고 있었다. 메모리에 db를 구성해서 cpu - memory 통신이 memmory - disk 통신보다 훨씬 빠르니까 일종의 캐시처럼 이용하는 인메모리 db 정도라는 감이 있었다. 사용해보니 nosql 구조였고 처음 겪어보는 nosql이라 신기했다.
<br> <br>
redis queue에 경우에 처음에는 생소했으나 사용하다보니 일종의 스케줄러같다는 느낌을 많이 받았다. 스케줄러인데 인스턴스가 일하는 것과 비동기적으로 일을 처리할 수 있는 스케줄러 느낌.

#### 작업

처음에 구현을 전체적으로 시작하기 전에 test용 및 앞으로 사용할 것 같은 기본 redis crud 기능들을 미리 찾아서 api로 구현했다. 사실 이건 그냥 함수 호출이라 어려운 부분은 없었다.
<br> <br>
처음에 worker라는 개념이 조금 와닿지 않았고 그리고 이를 코드 안에서 선언을 해서 런타임에 추가를 해야하는 것인지 아니면 따로 내 로컬에서 실행을 해주어야 하는지 잘 몰랐었다. 그래서 RQ 공식 문서를 읽어보며 조금 공부를 해보았는데, 로컬에서 실행할 경우 따로 터미널을 켜서 rq worker라는 명령어를 입력해야하는 것을 보니 컨테이너가 하나 더 있어야겠다라는 판단이 들어서 컨테이너 하나를 더 추가했다. 그리고 이는 docker compose 파일을 바꿔야하는 작업이었는데, docker compose 파일 또한 처음 겪어보는 것이라 처음에 설정하는 데에 애를 많이 먹었다. 가장 trouble shooting에 시간을 많이 쏟았던 부분이 이 worker 설정이었다. worker를 설정을 완료하고 redis queue로 일을 시키고 나서는 redis에 내가 저장하지 않았던 정보가 저장되어서 당황스러웠다. 이는 api를 여러번 쏴보고 result_ttl등 여러가지 정보를 확인해보고 내가 만든 api로 인해서 만들어진 key가 아닌 rq가 job을 처리하는 과정에서 큐 이름을 저장한다거나, result를 저장한다거나 하는 일들을 하는 것을 깨닫았다. 그래서 이를 적절히 처리했다.

### 동시성 / 병렬성

https://fastapi.tiangolo.com/ko/async/
<br> <br>
공식문서를 읽어보다가 이 부분을 봤는데 너무 재밌게 읽었다. 동기 / 비동기처리만 평상시에 어느정도 감을 잡고 있었는데, 병령설과 비동기의 차이를 재밌게 풀어 놓아서 읽어보았다. 해외 버전으로 보니까 이모지가 아니라 정말로 그림으로 설명으로 해두어서 신기했다. 개발문서에 이렇게 진심인게 놀라웠다.

### redis_conn.keys()

#### 도입

구현을 하는 도중에 google에 여러 레포를 보고 있었는데, redis에서 keys()를 사용하는 것을 그렇게 권장하지 않는다는 글을 봤다. 실제로 공식문서에도 keys()를 사용하는 것을 꺼려했다. 이유는 찾아보니 redis는 싱글 스레드 기반이고 keys()는 O(N)의 시간복잡도를 가지고 있어서 데이터가 많을 경우에 이 일을 처리하는 것 때문에 다른 일들을 처리하기 어려울 수 있기 때문이라고 한다. 그래서 scan() 이라고 잘라서 순회하는 함수가 있었는데 이것을 사용하는 것이 더 좋다고 했다.

#### 작업

나의 코드에서는 keys()를 자주 호출해야한다. 이것을 찾아보게 된 계기도 여러 함수에서 지속적으로 전체 key를 가져와야하는 문제가 있었기 때문이다. worker가 queue에 있는 일들을 처리하면서 어쩔 수 없는 몇가지 데이터를 남기게 되는 것 때문에 생긴 문제였다. 그래서 이를 후처리로 없앨까, 아니면 rq 기능중에 이를 꺼버리는 것이 가능하지 않을까, 아니면 그냥 냅둘까, 를 고민했다. 두번째 안의 경우에는 찾지 못해서 포기했다. 지우는 것을 고민하기 전에 우선 지워도 될까? 라는 고민을 해보았는데, result가 이 작업이 어떻게 실행되었는 지에 대한 로그를 남기는 것 같아서 좀 꺼림칙했다. 하지만 이를 지우지 않으면 50개만 생성되어야 하는 데이터가 150개나 생성이 되어 매번 keys() 함수를 불러올 때 3배나 많이 일을 해야하는 문제가 발생했다. 그래서 유저의 입장에서 더 빠른 속도를 제공하는 것이 올바르다고 판단해 지우기로 결정했다. 차라리 실패를 하게되면 실패한 것에 대한 call back 함수를 이용할 수 있던데, 그렇게 실패를 한 부분만 call back 하게 하는 것이 올바를 것 같다. 첫번째 안의 경우에 10초 안에 일이 끝나도록 설계를 해놓았으니까 10초 뒤에 바로 데이터 후처리를 해버리면 되지 않을까..? 라는 생각을 해보았는데, 항상 10초에 딱 일이 끝나는 것도 아니기도하고, 10초가 되기 전에 비동기적으로 다른 api로 redis에 접근을 하게 되면 오류가 발생할 것이기 때문에 그냥 냅둔 상태에서 항상 api에 오류가 발생하지 않도록 만들게 설계했다.

### exception 처리

유저 측면에서 delete 실패와 객체를 가져오는 것에 대하여 만약에 실패했을 때 어떻게 처리할 지를 고민했다. 랜덤으로 나온 수보다 객체 수가 적을 경우에 예외를 던져 실패했다고 표시, 가져올 객체가 넣은 cnt보다 적을 경우에도 예외를 던져 실패했다고 표시 할 수 있다. 아니면 그냥 가능한 최대로 상황을 해결하여 예외를 발생시키지 않는 방법도 있었는데, 나는 유저가 해당 api를 호출 했을 때 어지간하면 유저의 의도를 인정해주고 싶어서 exception을 처리하지 않고 후자의 방향으로 구현했다. 이렇게 디자인 방향을 잡은 까닭은 예전에 배운 Thrashing 현상에 대한 교수님의 이야기 때문이었다. 멀티 프로세스 환경에서 한 프로세스 당 할당해 줄 페이지가 많이 적어지면 페이지 폴트 비율이 많이 증가해서 프로세스 들이 전부 심각하게 느려지는 현상을 쓰레싱이라고 한다 배웠다. 하지만 대다수의 운영체제에서 이 쓰레싱을 막기위한 조치를 따로 하지 않는다고 그러셨다. " 유저가 프로그램을 더블클릭 하면 켜지는 것이 우선이다 " 라고 말씀하셨고, 결과는 유저가 책임을 지는 것이다 라고 하셨다. 그 기억에 나는 어지간하면 유저의 의도를 인정해주고 싶어서 이런 방향으로 디자인했다.

## 추가 평가항목 관련

#### 이런 구조를 왜 비동기 패턴이라고 하고, 왜 대용량 처리에서 비동기 방식을 선호하는지 설명할 수 있으면 좋습니다.

비동기(Asynchronous) 처리란 요청을 보내고 해당 요청이 끝날 때까지 기다리지 않고 다른 일을 수행할 수 있게 일을 처리하는 방식이다. 현재 redis queue에게 일을 처리하라고 명령을 내리면 서버는 바로 그 함수 호출을 끝내고 다른 일이 가능한 상태가 된다. worker는 다른 컨테이너에서 job들을 처리하고 있지만 그것을 처리하는 동안 서버가 기다리지 않는다. 그래서 worker가 일을 하는 동안 다른 api들을 호출하거나 gen value함수를 다시 호출해도 정상적으로 서버가 동작한다. 이는 그래서 비동기 패턴으로 볼 수 있다.

왜 대용량 처리에서 비동기 방식을 선호하는 지에 대하여 많이 찾아 보기도 하고 생각을 해봤는데, 혼자 좀 생각을 해본 결과 운영체제에서 발생시키는 interrupt와 비슷하다고 생각이 들었다. 우리가 disk I/O 관련으로 system call을 발생시킬 때, 일반적으로 디스크에 I/O 작업을 하는 것은 오래 걸리기 때문에 이것이 끝날 때까지 기다리게 된다면 CPU는 놀아야 한다. CPU가 쓸모없이 노는 현상은 죄악이기 때문에 우리는 이를 이렇게 처리하지 않고 오래 걸리는 일을 시켜두고 돌아와서 다른 일을 진행하다가 I/O 작업이 끝나면 해당 디바이스가 interrupt를 보내 OS가 이를 마무리 할 수 있게 한다. 이와 마찬가지로 비동기 처리에서 생각을 해보면, 용량이 작아서 빠르게 일을 처리할 수 있는 일은 동기 처리로 진행을 하더라도 속도가 받쳐주기 때문에 리소스가 그렇게 오래 쉴 일도 없고 유저가 느끼는 지연율도 낮다. 하지만 대용량을 처리해야하는 상황에서는 일 하나를 처리하기 위해서 오랜 시간이 걸리게 되고, 또 이 오래 걸리는 일들을 여러 사용자가 동시 다발적으로 발생시키면 이것 때문에 서버의 성능이 엄청 느려질 수 있다. OS에서도 interrupt를 사용하게 된 배경이 disk I/O 작업과 같은 오래 걸리는 일이었던 것처럼 비동기 처리는 이러한 대용량 처리에서 사용자들이 느끼기에 서버의 속도를 빠르게 하고 더 나은 경험을 제공할 수 있다.

#### redis queue의 워커로 pytorch inference 모듈이 연결되어 있다면, 배치 구성은 어떻게 하는 것이 좋을지 제안해 주세요.

위에서도 언급했다시피 redis queue는 스케줄러와 비슷하게 생겼다는 생각을 했다. 그걸 바탕으로 좀 고민을 해봤다.

**<상황가정>**

설명의 편의를 위해서 pytorch inference로 이미지의 정보를 추론한다고 상황을 설정하겠다. 그리고 각 사진을 추론하는 데 걸리는 시간이 거의 동일하다고 가정하겠다.

**<나의 생각>**

1. **문제**

   추론을 요청하는 사진의 양이 다를 수 있다. 한 장만 추론을 해달라 할 수도 있고, 적게는 2,3장 많게는 100장, 200장을 추론해달라고 할 수 있다. 많은 양의 사진을 추론하는 것은 시간이 그 양에 비례하여 많이 걸리기 때문에 큐가 하나라면 100장, 200장을 추론하게 되면 그 뒤에 처리하게 될 일들은 그만큼 오래 기다려야 한다.

   그리고 적은 양의 이미지를 추론하는 일들을 여러번 수행하는 것도 효율적이지 못하다.

2. **해결 방안**

   - 한 번에 enqueue 가능한 이미지 양을 정함. (ex 10장)
   - 10장 보다 사진이 적은 요청이 들어온다면 우선 기다림
   - 여러 요청들이 모여 10장이 되었다면 enqueue

   이렇게만 한다면 여러가지 문제가 발생한다.

   - 만약 10장이 되지 않았는데 다른 요청이 더 들어오지 않는다면 계속 기다려야함
   - 요청 자체가 10장 보다 더 많은 요청이 들어오면 enqueue 불가능

   그래서 이런 생각을 추가로 더 해보았다.

   - time limit tick을 정해서 그 시간 안에 다른 요청이 더 들어오지 않는다면 10장이 안되어도 enqueue (Round Robin)
   - 10장 보다 더 많으면 그 일을 10개씩 나누어서 일을 처리

   그런데 이렇게 해도 긴 작업들 때문에 다른 작업이 밀릴 수 있다. 이럴 때는

   - 기본적으로 round-robin 형식은 지킴
   - 가능하다면? job 별로 priority를 부여
   - 오래된 작업들은 priority boost를 통해 먼저 일을 할 수 있게 앞으로 당겨줌 (사진이 많아 나누어 진 일들도 하나의 일로 취급)
   - worker를 다중으로 설정하고 priority 마다 다른 worker에 넣는 전략을 사용 (multilevel feedback queue)
   - 여러개의 worker는 비동기적으로 일을 처리할 수 있으므로 효율이 좋을 것으로 예상됨
   - 하지만 priority boost와 같은 로직 처리와 priority를 모두 가지고 있어야하는 overhead가 발생하므로, 득실을 잘 따져보아야 함
